import os
from langchain_community.llms import Ollama
from langchain_core.prompts import PromptTemplate
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_community.cache import InMemoryCache
import langchain

# ====== Cáº¥u hÃ¬nh cache Ä‘á»ƒ tÄƒng tá»‘c ======
langchain.llm_cache = InMemoryCache()

# ====== Cáº¥u hÃ¬nh chatbot ======
DATA_PATH = "kien_thuc_giao_duc.txt"
CHROMA_DIR = "data/chroma_db"
OLLAMA_BASE = "http://localhost:11434"
EMBED_MODEL = "nomic-embed-text"
LLM_MODEL = "gemma2:9b"

# ====== 1) Load dá»¯ liá»‡u ======
if not os.path.exists(DATA_PATH):
    raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u: {DATA_PATH}")

print("ğŸ“˜ Äang táº£i dá»¯ liá»‡u...")
loader = TextLoader(DATA_PATH, encoding="utf-8")
documents = loader.load()

# ====== 2) Chia nhá» vÄƒn báº£n ======
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = splitter.split_documents(documents)
print(f"âœ… ÄÃ£ chia thÃ nh {len(chunks)} Ä‘oáº¡n.")

# ====== 3) Táº¡o embeddings + Chroma vectorstore ======
print("ğŸ”¢ Äang táº¡o embeddings...")
embeddings = OllamaEmbeddings(model=EMBED_MODEL, base_url=OLLAMA_BASE)

vectorstore = Chroma.from_documents(chunks, embedding=embeddings, persist_directory=CHROMA_DIR)
vectorstore.persist()
print("ğŸ’¾ Vectorstore Ä‘Ã£ sáºµn sÃ ng.")

# ====== 4) Khá»Ÿi táº¡o LLM ======
llm = Ollama(model=LLM_MODEL, base_url=OLLAMA_BASE)

# ====== 5) Prompt Template ======
EDU_PROMPT = (
    "Báº¡n lÃ  trá»£ lÃ½ áº£o cá»§a TrÆ°á»ng Äáº¡i há»c Cáº§n ThÆ¡ (CTU), chuyÃªn há»— trá»£ sinh viÃªn vá» há»c vá»¥ vÃ  hÃ nh chÃ­nh. "
    "Chá»‰ tráº£ lá»i cÃ¡c cÃ¢u há»i liÃªn quan Ä‘áº¿n sinh viÃªn, tuyá»ƒn sinh, há»c vá»¥, há»c phÃ­, Ä‘Äƒng kÃ½ há»c pháº§n, quy Ä‘á»‹nh, "
    "vÃ  thÃ´ng tin liÃªn há»‡ trong trÆ°á»ng. "
    "Náº¿u cÃ¢u há»i náº±m ngoÃ i cÃ¡c lÄ©nh vá»±c nÃ y (vÃ­ dá»¥: thá»i sá»±, láº­p trÃ¬nh, thá»i tiáº¿t, giáº£i trÃ­, chÃ­nh trá»‹...), "
    "hÃ£y tráº£ lá»i: 'Xin lá»—i, tÃ´i chá»‰ há»— trá»£ thÃ´ng tin liÃªn quan Ä‘áº¿n há»c táº­p vÃ  sinh viÃªn TrÆ°á»ng Äáº¡i há»c Cáº§n ThÆ¡.' "
    "\n\nDá»±a trÃªn ngá»¯ cáº£nh Ä‘Æ°á»£c cung cáº¥p, hÃ£y tráº£ lá»i ngáº¯n gá»n, chÃ­nh xÃ¡c vÃ  báº±ng tiáº¿ng Viá»‡t thÃ¢n thiá»‡n."
)
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=EDU_PROMPT + "\n\nNgá»¯ cáº£nh:\n{context}\n\nCÃ¢u há»i: {question}\nTráº£ lá»i:"
)

# ====== 6) VÃ²ng láº·p há»i Ä‘Ã¡p ======
print("\nğŸ“ Chatbot sinh viÃªn CTU Ä‘Ã£ sáºµn sÃ ng! (gÃµ 'exit' Ä‘á»ƒ thoÃ¡t)\n")

while True:
    q = input("ğŸ‘©â€ğŸ“ Báº¡n: ").strip()
    if q.lower() == "exit":
        print("ğŸ‘‹ Táº¡m biá»‡t! TÃ´i ráº¥t vui vÃ¬ Ä‘Ã£ há»— trá»£ báº¡n.")
        break

    try:
        # 6.1 TÃ¬m cÃ¡c Ä‘oáº¡n liÃªn quan
        top_k = 3
        results = vectorstore.similarity_search(q, k=top_k)
        context = "\n\n".join([doc.page_content for doc in results]) if results else ""

        # Náº¿u khÃ´ng cÃ³ dá»¯ liá»‡u phÃ¹ há»£p
        if not context:
            print("ğŸ¤– Trá»£ lÃ½ CTU: TÃ´i chÆ°a cÃ³ dá»¯ liá»‡u vá» ná»™i dung nÃ y.\n")
            continue

        # 6.2 GhÃ©p prompt
        final_prompt = prompt.format(context=context, question=q)

        # 6.3 Gá»i LLM
        answer = llm.invoke(final_prompt)
        print(f"ğŸ¤– Trá»£ lÃ½ CTU: {answer}\n")

    except Exception as e:
        print(f"âš ï¸ Lá»—i khi xá»­ lÃ½ cÃ¢u há»i: {e}\n")
